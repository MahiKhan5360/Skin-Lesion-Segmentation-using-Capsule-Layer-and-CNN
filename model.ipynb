{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahiKhan5360/Skin-Lesion-Segmentation-using-Capsule-Layer-and-CNN/blob/main/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOrnHpBqJcWf"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, BatchNormalization, UpSampling2D, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.metrics import MeanIoU\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Capsule Layer Class\n",
        "def create_capsule_layer():\n",
        "    class CapsuleLayer(tf.keras.layers.Layer):\n",
        "        def __init__(self, num_capsules=4, capsule_dim=8, num_routing=2, **kwargs):\n",
        "            super(CapsuleLayer, self).__init__(**kwargs)\n",
        "            self.num_capsules = num_capsules\n",
        "            self.capsule_dim = capsule_dim\n",
        "            self.num_routing = num_routing\n",
        "\n",
        "        def build(self, input_shape):\n",
        "            self.input_channels = input_shape[-1]\n",
        "\n",
        "            self.W = self.add_weight(\n",
        "                shape=[self.input_channels, self.num_capsules * self.capsule_dim],\n",
        "                initializer='glorot_uniform',\n",
        "                trainable=True\n",
        "            )\n",
        "            super().build(input_shape)\n",
        "\n",
        "        def compute_output_shape(self, input_shape):\n",
        "\n",
        "            return (input_shape[0], input_shape[1], input_shape[2], self.capsule_dim)\n",
        "\n",
        "        def call(self, inputs):\n",
        "            batch_size = tf.shape(inputs)[0]\n",
        "            height, width = inputs.shape[1], inputs.shape[2]\n",
        "\n",
        "            inputs_activated = tf.nn.relu(inputs)\n",
        "\n",
        "\n",
        "            u_hat = tf.matmul(\n",
        "                tf.reshape(inputs_activated, [-1, self.input_channels]),\n",
        "                self.W\n",
        "            )\n",
        "            u_hat = tf.reshape(u_hat, [batch_size, height, width, self.num_capsules, self.capsule_dim])\n",
        "\n",
        "\n",
        "            b = tf.zeros([batch_size, height, width, self.num_capsules])\n",
        "\n",
        "            for i in range(self.num_routing):\n",
        "                # Softmax over capsules dimension\n",
        "                c = tf.nn.softmax(b, axis=-1)  # Shape: [batch, height, width, num_capsules]\n",
        "\n",
        "                # Expand c to match u_hat dimensions for element-wise multiplication\n",
        "                c_expanded = tf.expand_dims(c, axis=-1)  # Shape: [batch, height, width, num_capsules, 1]\n",
        "\n",
        "\n",
        "                s = tf.reduce_sum(c_expanded * u_hat, axis=3)  # Sum over num_capsules\n",
        "                # s shape: [batch, height, width, capsule_dim]\n",
        "\n",
        "                # Apply squashing\n",
        "                v = self.squash(s)\n",
        "\n",
        "                # Update routing coefficients if not last iteration\n",
        "                if i < self.num_routing - 1:\n",
        "                    # v shape: [batch, height, width, capsule_dim]\n",
        "                    # u_hat shape: [batch, height, width, num_capsules, capsule_dim]\n",
        "                    v_expanded = tf.expand_dims(v, axis=3)  # [batch, height, width, 1, capsule_dim]\n",
        "\n",
        "                    # Calculate agreement: dot product between v and each u_hat\n",
        "                    agreement = tf.reduce_sum(u_hat * v_expanded, axis=-1)  # [batch, height, width, num_capsules]\n",
        "                    b = b + agreement\n",
        "\n",
        "            return v\n",
        "\n",
        "        def squash(self, s, axis=-1, epsilon=1e-9):\n",
        "            s_squared_norm = tf.reduce_sum(tf.square(s), axis=axis, keepdims=True)\n",
        "            scale = s_squared_norm / (1 + s_squared_norm + epsilon)\n",
        "            return scale * s / tf.sqrt(s_squared_norm + epsilon)\n",
        "\n",
        "    return CapsuleLayer\n",
        "\n",
        "# Memory-efficient Model Creation\n",
        "def create_efficient_capsule_segmentation_model(input_shape=(256, 256, 3), num_capsules=4, capsule_dim=8):\n",
        "    CapsuleLayer = create_capsule_layer()\n",
        "\n",
        "    # Input layer\n",
        "    input_layer = Input(shape=input_shape)\n",
        "\n",
        "\n",
        "    # Encoder:\n",
        "    # Block 1\n",
        "    x = Conv2D(filters=32, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(0.0001))(input_layer)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(filters=32, kernel_size=3, activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = Dropout(0.2)(x)  # Reduced dropout\n",
        "\n",
        "    # Block 2\n",
        "    x = Conv2D(filters=64, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(0.0001))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(filters=64, kernel_size=3, activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    # Block 3 - Additional downsampling to reduce spatial dimension\n",
        "    x = Conv2D(filters=128, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(0.0001))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    # Capsule layer with reduced parameters\n",
        "    x = CapsuleLayer(num_capsules=num_capsules, capsule_dim=capsule_dim)(x)\n",
        "\n",
        "     # Decoder: upsampling\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    x = Conv2D(filters=64, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(0.0001))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    x = Conv2D(filters=32, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(0.0001))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    x = Conv2D(filters=16, kernel_size=3, activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    # Output layer\n",
        "    output_layer = Conv2D(filters=1, kernel_size=1, activation='sigmoid', padding='same')(x)\n",
        "\n",
        "    # model\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "    # Compile with memory efficient optimizer settings\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001, clipnorm=1.0),  # Added gradient clipping\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy', MeanIoU(num_classes=2)]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Memory management utility\n",
        "def clear_memory():\n",
        "    tf.keras.backend.clear_session()\n",
        "    gc.collect()\n",
        "\n",
        "# Memory-efficient training function\n",
        "def train_model_with_memory_management(model, train_dataset, val_dataset, epochs=50):\n",
        "    # Memory-efficient callbacks\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, verbose=1),\n",
        "        ModelCheckpoint(\n",
        "            filepath='/content/drive/MyDrive/ISIC2018/best_capsule_model.keras',\n",
        "            save_best_only=True,\n",
        "            monitor='val_loss',\n",
        "            save_weights_only=False,  # Save full model- less frequently\n",
        "            verbose=1\n",
        "        ),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=4, min_lr=1e-7, verbose=1)\n",
        "    ]\n",
        "\n",
        "    # Train with memory management\n",
        "    print(\"Training memory-efficient model...\")\n",
        "    try:\n",
        "        history = model.fit(\n",
        "            train_dataset,\n",
        "            validation_data=val_dataset,\n",
        "            epochs=epochs,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "        return history\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Training interrupted: {e}\")\n",
        "        clear_memory()\n",
        "        return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Clear any existing sessions\n",
        "    clear_memory()\n",
        "\n",
        "    # Create model with reduced parameters\n",
        "    print(\"Creating memory-efficient model...\")\n",
        "    model = create_efficient_capsule_segmentation_model(\n",
        "        input_shape=(256, 256, 3),\n",
        "        num_capsules=4,  # Reduced from 8\n",
        "        capsule_dim=8    # Reduced from 16\n",
        "    )\n",
        "\n",
        "    print(\"Model created successfully!\")\n",
        "    model.summary()\n",
        "\n",
        "    # Train with memory management\n",
        "    history = train_model_with_memory_management(model, train_dataset, val_dataset, epochs=50)\n",
        "\n",
        "    if history is not None:\n",
        "        # Evaluate model on test set\n",
        "        print(\"\\nEvaluating model on test set...\")\n",
        "        try:\n",
        "            test_results = model.evaluate(test_dataset, verbose=1)\n",
        "            test_loss, test_accuracy, test_iou = test_results\n",
        "            print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, Test IoU: {test_iou:.4f}\")\n",
        "\n",
        "            # Save final model\n",
        "            model.save('/content/drive/MyDrive/ISIC2018/final_efficient_capsule_model.keras')\n",
        "            print(\"Model saved successfully!\")"
      ],
      "metadata": {
        "id": "TyEuCPzLJ0t0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}